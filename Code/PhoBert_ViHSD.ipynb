{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd D:\\Project\\Toolkit_for_Preprocessing_MXH\\viHSD_tokenize\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "# clear gpu memory using torch\n",
    "torch.cuda.empty_cache()\n",
    "# clear output\n",
    "clear_output()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = (r\"Data\\train.csv\")\n",
    "dev_path = (r\"Data\\dev.csv\")\n",
    "test_path = (r\"Data\\test.csv\")\n",
    "model_name = 'vinai/phobert-base'\n",
    "test_index = 50 # default None value\n",
    "batch_size = 4\n",
    "max_len = 128\n",
    "lr = 5e-6\n",
    "epochs = 2\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 35 samples\n",
      "Test set: 7 samples\n",
      "Development set: 8 samples\n"
     ]
    }
   ],
   "source": [
    "from Code.Dataset import split_path, create_dataloader\n",
    "\n",
    "if test_index != None and test_index > 3:\n",
    "    # Load the data\n",
    "    train_path, dev_path, test_path = split_path(test_path, test_index, train_path, dev_path, test_path)\n",
    "elif test_index != None: \n",
    "    print(\"Test index out of range. Please provide a valid interger index greater than 3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(64001, 768, padding_idx=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer, AutoModel\n",
    ")\n",
    "\n",
    "\n",
    "classes = ['0', '1', '2']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name)\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(classes)  # Number of classes\n",
    ")\n",
    "clear_output()\n",
    "\n",
    "# Adjust the token embeddings size if needed\n",
    "embedding_model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['index', 'Emotion', 'Text', 'Tag', 'sentence_id'], dtype='object')\n",
      "Columns: Index(['index', 'Emotion', 'Text', 'Tag', 'sentence_id'], dtype='object')\n",
      "Columns: Index(['index', 'Emotion', 'Text', 'Tag', 'sentence_id'], dtype='object')\n",
      "Input IDs: torch.Size([4, 128])\n",
      "Attention Mask: torch.Size([4, 128])\n",
      "Labels: torch.Size([4, 7])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = create_dataloader(train_path, batch_size=batch_size, tokenizer = tokenizer, max_len=max_len, shuffle=False)\n",
    "dev_dataloader = create_dataloader(dev_path, batch_size=batch_size, tokenizer = tokenizer, max_len=max_len, shuffle=False)\n",
    "test_dataloader = create_dataloader(test_path, batch_size=batch_size, tokenizer = tokenizer, max_len=max_len, shuffle=False)\n",
    "\n",
    "# Get the first batch of data from the DataLoader\n",
    "first_batch = next(iter(test_dataloader))\n",
    "\n",
    "# Access input_ids, attention_mask, and labels\n",
    "input_ids = first_batch['input_ids']\n",
    "attention_mask = first_batch['attention_mask']\n",
    "labels = first_batch['label']\n",
    "\n",
    "# Print to check\n",
    "print(f\"Input IDs: {input_ids.size()}\")\n",
    "print(f\"Attention Mask: {attention_mask.size()}\")\n",
    "print(f\"Labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model\n",
    "from Code.Model import setup_model, MultiTaskModel, train, test\n",
    "\n",
    "model, criterion, optimizer, device, num_epochs = setup_model(\n",
    "    model_class = MultiTaskModel, \n",
    "    embedding_model = embedding_model, \n",
    "    classification_model = classification_model,\n",
    "    lr=5e-6,\n",
    "    weight_decay=1e-5,\n",
    "    num_epochs=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.9249\n",
      "Validation Loss: 1.9834\n",
      "Macro F1-Score: 0.4615\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.8486\n",
      "Validation Loss: 1.9621\n",
      "Macro F1-Score: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Assuming you've set up the model, dataloaders, criterion, and optimizer\n",
    "train(model, train_dataloader, dev_dataloader, criterion, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2/2 [00:01<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions, true_labels = test(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd D:\\Project\\Toolkit_for_Preprocessing_MXH\\VSMEC_tokenize\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training set: 35 samples\n",
      "Test set: 7 samples\n",
      "Development set: 8 samples\n",
      "Columns: Index(['index', 'Text', 'Tag', 'sentence_id'], dtype='object')\n",
      "Columns: Index(['index', 'Text', 'Tag', 'sentence_id'], dtype='object')\n",
      "Columns: Index(['index', 'Text', 'Tag', 'sentence_id'], dtype='object')\n",
      "Input IDs: torch.Size([6, 128])\n",
      "Attention Mask: torch.Size([6, 128])\n",
      "Labels: torch.Size([6, 3])\n",
      "Epoch: 1\n",
      "Training Loss: 1.0536\n",
      "Validation Loss: 1.0308\n",
      "Macro F1-Score: 0.4000\n",
      "Epoch: 2\n",
      "Training Loss: 0.9967\n",
      "Validation Loss: 0.9750\n",
      "Macro F1-Score: 0.4000\n",
      "Macro F1 Score: 0.4000\n",
      "Test results saved to test_results.json\n",
      "Model saved to output\\trained_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndp17\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\ndp17\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Training:  20%|██        | 1/5 [00:10<00:42, 10.61s/it]\n",
      "Training:  40%|████      | 2/5 [00:18<00:26,  8.72s/it]\n",
      "Training:  60%|██████    | 3/5 [00:25<00:16,  8.07s/it]\n",
      "Training:  80%|████████  | 4/5 [00:32<00:07,  7.83s/it]\n",
      "Training: 100%|██████████| 5/5 [00:36<00:00,  6.21s/it]\n",
      "                                                       \n",
      "\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Validation: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "                                                         \n",
      "\n",
      "Training:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Training:  20%|██        | 1/5 [00:07<00:30,  7.63s/it]\n",
      "Training:  40%|████      | 2/5 [00:15<00:22,  7.59s/it]\n",
      "Training:  60%|██████    | 3/5 [00:22<00:15,  7.60s/it]\n",
      "Training:  80%|████████  | 4/5 [00:30<00:07,  7.60s/it]\n",
      "Training: 100%|██████████| 5/5 [00:33<00:00,  6.08s/it]\n",
      "                                                       \n",
      "\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Validation: 100%|██████████| 1/1 [00:03<00:00,  3.20s/it]\n",
      "                                                         \n",
      "\n",
      "Testing:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Testing: 100%|██████████| 1/1 [00:02<00:00,  2.64s/it]\n",
      "Testing: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n"
     ]
    }
   ],
   "source": [
    "!python main.py --model \"vinai/phobert-base\" --train_path \"Data\\train.csv\" --dev_path \"Data\\dev.csv\" --test_path \"Data\\test.csv\" --batch_size 8 --max_len 128 --lr 5e-6 --num_epochs 2 --output_json \"test_results.json\" --output_dir \"output\" --test_index 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
